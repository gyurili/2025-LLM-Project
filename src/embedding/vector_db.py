import os
from typing import List, Union, Optional, Literal

import faiss
from dotenv import load_dotenv
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_chroma import Chroma
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_openai import OpenAIEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores.base import VectorStore


from src.utils.path import get_project_root_dir

def generate_embedding(embed_model_name: str) -> Union[OpenAIEmbeddings, HuggingFaceEmbeddings]:
    """
    ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    Args:
        embed_model_name (str): 'openai' ë˜ëŠ” huggingface ëª¨ë¸ ì´ë¦„

    Returns:
        ì„ë² ë”© ê°ì²´

    Raises:
        ValueError: ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
    """
    try:
        if embed_model_name == "openai":
            load_dotenv()
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("âŒ OPENAI_API_KEYê°€ .envì— ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return OpenAIEmbeddings(model="text-embedding-3-large", openai_api_key=api_key)
        else:
            return HuggingFaceEmbeddings(model_name=embed_model_name)
    except Exception as e:
        raise ValueError(f"âŒ [Value] (vector_db.generate_embedding) ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

def generate_vector_db(
        all_chunks: List[Document], 
        embed_model_name: str,
        index_name: str,
        db_type: str = "faiss",
        is_save: bool = True,
        batch_size:int = 128
    ) -> Union[FAISS, Chroma]:
    """
    FAISS ë˜ëŠ” Chroma ê¸°ë°˜ ë²¡í„° DBë¥¼ ìƒì„±í•˜ê³  ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Note:
        Chromaì˜ ê²½ìš°, PersistentClientë¥¼ ì‚¬ìš©í•  ë•Œ, ì§€ì •ëœ 
        ë””ë ‰í† ë¦¬ì— ì´ë¯¸ ì €ì¥ëœ DBê°€ ìˆì„ ê²½ìš° ë®ì–´ì“°ê¸° ì‹œ ì˜¤ë¥˜ê°€ ìƒê¸´ë‹¤.
        -> ë®ì–´ì“°ê¸° ëŒ€ì‹ , ë§¤ ì‹¤í–‰ì‹œ, ìƒì„±ë˜ëŠ” í´ë”ë¥¼ ì§€ì • ì œê±° í›„ ìƒì„± ì§„í–‰.

    Args:
        all_chunks (List[Document]): ì²­í¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        embed_model_name (str): ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        index_name (str): ë²¡í„° DB ì¸ë±ìŠ¤ ì´ë¦„
        db_type (str): ì‚¬ìš©í•  ë²¡í„° DB íƒ€ì… ('faiss' ë˜ëŠ” 'chroma')
        is_save (bool): ë§Œë“¤ì–´ì§„ vector dbë¥¼ ì €ì¥ í• ê±´ì§€
        batch_size (int): documentì…ë ¥ ì‹œ í•œë²ˆì— ì¶”ê°€ ë  documentì˜ ì‚¬ì´ì¦ˆ(batch size)

    Returns:
        VectorStore ê°ì²´

    Raises:
        ValueError: ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
    """
    if not all_chunks or not isinstance(all_chunks, list):
        raise ValueError("âŒ [Value] (vector_db.generate_vector_db) all_chunksëŠ” ë¹„ì–´ ìˆì§€ ì•Šì€ Document ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    embeddings = generate_embedding(embed_model_name)
    print(f"ğŸ“Œ [Info] Embedding model: {embeddings.__class__.__name__}")
    
    try:
        if isinstance(embeddings, HuggingFaceEmbeddings):
            dimension = len(embeddings.embed_query("hello world"))
        elif isinstance(embeddings, OpenAIEmbeddings):
            dimension = len(embeddings.embed_query("hello world"))
        else:
            dimension = 1536 # OpenAIì˜ ê²½ìš° .embed_query ë¹„ìš© ë°œìƒ, ìˆ˜ë™ ì…ë ¥ìœ¼ë¡œ ë¹„ìš© ì ˆê°.

    except Exception as e:
        raise ValueError(f"âŒ [Value] (vector_db.generate_vector_db) ì„ë² ë”© ì°¨ì› ê³„ì‚°ì„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.: {e}")

    try:
        output_path = os.path.join(get_project_root_dir(), "data")
        if not os.path.exists(output_path):
            os.makedirs(output_path, exist_ok=True)
        
        if db_type == "faiss":
            print(f"ğŸ“Œ [info] Vector DB type: {db_type}")
            vector_store = FAISS(
                embedding_function=embeddings,
                index=faiss.IndexFlatL2(dimension),
                docstore=InMemoryDocstore(),
                index_to_docstore_id={}
            )
            vector_store = add_docs_in_batch(vector_store, all_chunks)
            if is_save:
                vector_store.save_local(folder_path=output_path, index_name=index_name)
                print("âœ… [Success] FAISS vector DB ì €ì¥ ì„±ê³µ.")
                
        elif db_type == "chroma":
            print(f"ğŸ“Œ [info] Vector DB type: {db_type}")
            chroma_path = os.path.join(output_path, index_name)

            if os.path.exists(chroma_path):
                import shutil
                shutil.rmtree(chroma_path)
                print(f"    âš ï¸ [Notification] {db_type} ë®ì–´ì“°ê¸° ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì´ì „ ìƒì„± DB ì œê±°...")
                
            vector_store = Chroma(
                embedding_function=embeddings,
                persist_directory=chroma_path,
                collection_name="chroma_db"
            )

            vector_store = add_docs_in_batch(vector_store, all_chunks)
            print("    âœ… [Success] Chroma vector DB ì €ì¥ ì„±ê³µ.")
        else:
            raise ValueError("âŒ [Value] (vector_db.generate_vector_db) ì§€ì›í•˜ì§€ ì•ŠëŠ” ë²¡í„° DB íƒ€ì…ì…ë‹ˆë‹¤. ('faiss' ë˜ëŠ” 'chroma' ì‚¬ìš©)")
        
        return vector_store
    
    except Exception as e:
        raise RuntimeError(f"âŒ [Runtime] (vector_db.generate_vector_db) ë²¡í„° DB ìƒì„± ì‹¤íŒ¨: {e}")

def load_vector_db(
        path: str, 
        embed_model_name: str,
        index_name: str,
        db_type: str = "faiss"
    ) -> Union[FAISS, Chroma]:
    """
    ë¡œì»¬ì—ì„œ ì €ì¥ëœ FAISS ë²¡í„° DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        path (str): ë²¡í„° DBê°€ ì €ì¥ëœ ë£¨íŠ¸ ê²½ë¡œ (ì˜ˆ: "data")
        embed_model_name (str): ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        index_name (str): ë²¡í„° DB ì¸ë±ìŠ¤ ì´ë¦„
        db_type (str): ë²¡í„° DB íƒ€ì… ('faiss' ë˜ëŠ” 'chroma')

    Returns:
        VectorStore ê°ì²´

    Raises:
        FileNotFoundError: ê²½ë¡œê°€ ì—†ì„ ê²½ìš°
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ [FileNotFound] (vector_db.load_vector_db.path) ë²¡í„° DB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")

    try:
        embeddings = generate_embedding(embed_model_name)

        if db_type == "faiss":
            return FAISS.load_local(
                folder_path=path,
                index_name=index_name,
                embeddings=embeddings,
                allow_dangerous_deserialization=True,
            )
        elif db_type == "chroma":
            chroma_path = os.path.join(path, index_name)
            return Chroma(
                embedding_function=embeddings,
                persist_directory=chroma_path,
                collection_name="chroma_db"
            )
        else:
            raise ValueError(f"âŒ [Value] (vector_db.load_vector_db) ì§€ì›í•˜ì§€ ì•ŠëŠ” ë²¡í„° DB íƒ€ì…ì…ë‹ˆë‹¤. {db_type}")
        
    except Exception as e:
        raise RuntimeError(f"âŒ [Runtime] (vector_db.load_vector_db) ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")

from tqdm import tqdm

def add_docs_in_batch(vector_store:VectorStore,
                    chunks:Optional[List[Document]], 
                    batch_size:int=128):
    """
    ë¬¸ì„œ chunkë¥¼ batchë³„ ì¶”ê°€ í•˜ëŠ” ë°©ì‹.

    Args:
        vector_store: FAISS ë˜ëŠ” Chroma ì¸ìŠ¤í„´ìŠ¤
        chunks (List[Document]): chunk_sizeë¡œ ë‚˜ë‰˜ì–´ì§„ ë¬¸ì„œì˜ ë¦¬ìŠ¤íŠ¸
        batch_size (int): batch ì‚¬ì´ì¦ˆ

    Returns:
        vector_store: ë¬¸ì„œê°€ ì €ì¥ëœ vector dbë¥¼ ë°˜í™˜.
    """
    total = len(chunks)
    pbar = tqdm(
        range(0, total, batch_size), 
        desc=f"    ğŸ“Œ Indexing chunks to {vector_store.__class__.__name__}",
        unit="batch"
    )

    for i in pbar:
        batch = chunks[i:i + batch_size]
        vector_store.add_documents(batch)

        end_idx = min(i + batch_size, total)
        pbar.set_postfix_str(f"Indexed {end_idx} / {total}")
        
    return vector_store