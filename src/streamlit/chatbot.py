# í„°ë¯¸ë„ ì‹¤í–‰ ì½”ë“œ
# python -m streamlit run src/streamlit/chatbot.py

# ì™¸ë¶€ ì„í¬íŠ¸
import os
import time 
import streamlit as st
import shutil
from pathlib import Path
from datetime import datetime
os.environ["HF_HOME"] = "2025-LLM-Project/.cache" # Huggingface ìºì‹œ ê²½ë¡œ ì„¤ì •

# ë‚´ë¶€ ì„í¬íŠ¸
from dotenv import load_dotenv
from src.utils.config import load_config
from src.loader.loader_main import loader_main
from src.utils.path import get_project_root_dir
from src.embedding.embedding_main import embedding_main
from src.retrieval.retrieval_main import retrieval_main
from src.generator.generator_main import generator_main
from src.embedding.embedding_main import generate_index_name

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="RFP Chatbot", layout="wide") # (Chrome ìƒë‹¨ ë°” ì´ë¦„)
st.header("RFP Chatbot", divider='blue') # ìƒ‰ìƒ ì„ íƒ ("rainbow", "red", "blue", "green", "orange", "violet", "gray")
st.write("PDF, HWP í˜•ì‹ì˜ ì œì•ˆì„œë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë‚´ìš© ìš”ì•½ ë° ì§ˆì˜ì‘ë‹µì„ ê²½í—˜í•˜ì„¸ìš”!")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # [{"role": "user", "content": "..."}, {"role": "ai", "content": "..."}]

# ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
project_root = get_project_root_dir()
config_path = os.path.join(project_root, "config.yaml")
config = load_config(config_path)
dotenv_path = os.path.join(project_root, ".env")
load_dotenv(dotenv_path=dotenv_path)

# ì‚¬ì´ë“œ ë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    # Data ê´€ë ¨ ì„¤ì •
    st.subheader("ğŸ“‚ ë°ì´í„° ì„¤ì •")
    config["data"]["top_k"] = st.slider("ğŸ”¢ ìµœëŒ€ ë¬¸ì„œ ìˆ˜(files)", 1, 100, config["data"]["top_k"])
    config["data"]["file_type"] = st.selectbox("ğŸ“„ íŒŒì¼ ìœ í˜•", ["all", "pdf", "hwp"], index=["all", "pdf", "hwp"].index(config["data"]["file_type"]))
    config["data"]["apply_ocr"] = st.toggle("ğŸ§¾ OCR ì ìš© ì—¬ë¶€", config["data"]["apply_ocr"])
    config["data"]["splitter"] = st.selectbox("âœ‚ï¸ ë¬¸ì„œ ë¶„í•  ë°©ë²•", ["section", "recursive", "token"], index=["section", "recursive", "token"].index(config["data"]["splitter"]))
    config["data"]["chunk_size"] = st.number_input("ğŸ“ Chunk í¬ê¸°", value=config["data"]["chunk_size"], step=100)
    config["data"]["chunk_overlap"] = st.number_input("ğŸ” Chunk ì˜¤ë²„ë©", value=config["data"]["chunk_overlap"], step=10)

    # Embedding ì„¤ì •
    st.subheader("ğŸ§  ì„ë² ë”© ì„¤ì •")
    config["embedding"]["embed_model"] = st.text_input("ğŸ§¬ ì„ë² ë”© ëª¨ë¸", config["embedding"]["embed_model"])
    config["embedding"]["db_type"] = st.selectbox("ğŸ’¾ Vector DB íƒ€ì…", ["faiss", "chroma"], index=["faiss", "chroma"].index(config["embedding"]["db_type"]))

    if config["embedding"]["embed_model"].strip().lower() == "openai":
        load_dotenv()
        if not os.environ["OPENAI_API_KEY"]:
            openai_key = st.text_input("ğŸ”‘ OpenAI API Key", type="password")
            os.environ["OPENAI_API_KEY"] = openai_key
            if not openai_key:
                st.warning("OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ API í‚¤ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")

    # Retriever ì„¤ì •
    st.subheader("ğŸ” ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •")
    config["retriever"]["search_type"] = st.selectbox("ğŸ” ê²€ìƒ‰ ë°©ì‹", ["similarity", "hybrid"], index=["similarity", "hybrid"].index(config["retriever"]["search_type"]))
    config["retriever"]["top_k"] = st.slider("ğŸ“„ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜(chunks)", 1, 20, config["retriever"]["top_k"])
    config["retriever"]["rerank"] = st.toggle("ğŸ“Š ë¦¬ë­í¬ ì ìš©", config["retriever"]["rerank"])
    config["retriever"]["rerank_top_k"] = st.slider("ğŸ” ë¦¬ë­í¬ ë¬¸ì„œ ìˆ˜(chunks)", 1, 20, config["retriever"]["rerank_top_k"])

    # Generator ì„¤ì •
    st.subheader("ğŸ” ìƒì„±ì ì„¤ì •")
    config["generator"]["model_type"] = st.selectbox("ğŸ” ìƒì„± ëª¨ë¸ íƒ€ì…", ["huggingface", "openai"], index=["huggingface", "openai"].index(config["generator"]["model_type"]))
    config["generator"]["model_name"] = st.text_input("ğŸ§¬ ìƒì„± ëª¨ë¸", config["generator"]["model_name"])
    config["generator"]["max_length"] = st.number_input("ğŸ”¢ ìµœëŒ€ í† í° ìˆ˜(max_length)", value=config["generator"]["max_length"], step=32)

    if config["generator"]["model_type"].strip().lower() == "openai":
        load_dotenv()
        if not os.environ["OPENAI_API_KEY"]:
            openai_key = st.text_input("ğŸ”‘ OpenAI API Key", type="password")
            os.environ["OPENAI_API_KEY"] = openai_key  # í•„ìš”í•œ ê²½ìš° í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
            if not openai_key:
                st.warning("OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ API í‚¤ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")


    reset_vector_db = st.button("âš ï¸ Vector DB ì´ˆê¸°í™”")
    
    if config["embedding"]["db_type"] == "faiss":
        faiss_index_name = f"{generate_index_name(config)}"
        vector_db_file = os.path.join(project_root, 'data', f"{faiss_index_name}.faiss")
        metadata_file = os.path.join(project_root, 'data', f"{faiss_index_name}.pkl")
    else:
        chroma_folder_name = f"{generate_index_name(config)}"
        chroma_path = os.path.join(project_root, 'data', chroma_folder_name)


    if reset_vector_db:
        # ì„ íƒëœ ë²¡í„° DB ê²½ë¡œ ì‚­ì œ
        if config["embedding"]["db_type"] == "faiss":
            if os.path.exists(vector_db_file):
                os.remove(vector_db_file)
                os.remove(metadata_file)
                st.success("FAISS DB ì‚­ì œ ì™„ë£Œ")
        elif config["embedding"]["db_type"] == "chroma":
            import shutil
            if os.path.exists(chroma_path):
                shutil.rmtree(chroma_path)
                st.success("Chroma DB ì‚­ì œ ì™„ë£Œ")
        else:
            st.info("ì‚­ì œí•  íŒŒì¼ ë° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì±„íŒ…
query = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if query:
    # Vector DB ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if config["data"]["top_k"] == 100:
        if config["embedding"]["db_type"] == "faiss":
            is_save = not os.path.exists(vector_db_file)
        elif config["embedding"]["db_type"] == "chroma":
            is_save = not os.path.exists(chroma_path)
        else:
            is_save = True
    else:
        is_save = True

    # ì´ì „ ëŒ€í™”ë¡œ context êµ¬ì„±
    st.session_state.chat_history.append({"role": "user", "content": query})

    with st.chat_message("user"):
        st.markdown(query)

    config["retriever"]["query"] = query

    # ë°ì´í„° ì²˜ë¦¬
    chunks = loader_main(config)

    with st.spinner("ğŸ“‚ ê´€ë ¨ ë¬¸ì„œ ì„ë² ë”© ì¤‘..."):
        vector_store = embedding_main(config, chunks, is_save=is_save)

    with st.spinner("ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
        docs = retrieval_main(config, vector_store, chunks)

    # ì´ì „ ë¬¸ë§¥ì„ ì „ë‹¬í•˜ëŠ” ë°©ì‹ (ì„ íƒì‚¬í•­ - ëª¨ë¸ êµ¬í˜„ì— ë”°ë¼)
    config["chat_history"] = st.session_state.chat_history

    # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±, ì¶”ë¡  ì‹œê°„ ì¸¡ì •
    start_time = time.time()
    with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
        answer = generator_main(docs, config) # generator_main í•¨ìˆ˜ì— docsì™€ queryë¥¼ ì „ë‹¬
    end_time = time.time()
    elapsed = round(end_time - start_time, 2)

    # ì¶”ë¡  ê²°ê³¼, ì¶”ë¡  ì‹œê°„ í‘œì‹œ
    with st.chat_message("assistant"):
        st.markdown(answer)
        st.markdown(f"ğŸ•’ **ì¶”ë¡  ì‹œê°„:** {elapsed}ì´ˆ")


    # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
    st.session_state.chat_history.append({"role": "ai", "content": answer}) # ë‹µë³€ ê¸°ë¡ 

# ì´ì „ ëŒ€í™” ë³´ì—¬ì£¼ê¸°
# if st.session_state.chat_history:
#     st.markdown("### ëŒ€í™” ê¸°ë¡")
#     for turn in st.session_state.chat_history:
#         role = "ğŸ™‹â€â™‚ï¸ ì‚¬ìš©ì" if turn["role"] == "user" else "ğŸ¤– AI"
#         st.markdown(f"**{role}:** {turn['content']}")

# ì´ì „ ëŒ€í™” ë³´ì—¬ì£¼ê¸°(ì—…ë°ì´íŠ¸ ë²„ì „)
if st.session_state.chat_history:
    for turn in st.session_state.chat_history:
        with st.chat_message("user" if turn["role"] == "user" else "assistant"):
            st.markdown(turn["content"])
