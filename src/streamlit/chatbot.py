# í„°ë¯¸ë„ ì‹¤í–‰ ì½”ë“œ
# python -m streamlit run src/streamlit/chatbot.py

# ì™¸ë¶€ ì„í¬íŠ¸
import os
import time 
import shutil
import streamlit as st
from typing import Dict
from dotenv import load_dotenv

# ë‚´ë¶€ ì„í¬íŠ¸
from src.utils.config import load_config
from src.utils.path import get_project_root_dir
from src.utils.shared_cache import set_cache_dirs
from src.loader.loader_main import loader_main
from src.embedding.vector_db import generate_embedding
from src.embedding.embedding_main import embedding_main, generate_index_name
from src.retrieval.retrieval_main import retrieval_main
from src.generator.generator_main import generator_main
from src.generator.hf_generator import load_hf_model
from src.generator.openai_generator import load_openai_model
from src.generator.generator_main import load_chat_history
from main import rag_pipeline

set_cache_dirs()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RFP Chatbot", 
    layout="wide"
)

st.header("RFP Chatbot", divider='blue')
st.caption("PDF, HWP í˜•ì‹ì˜ ì œì•ˆì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‚´ìš© ìš”ì•½ ë° ì§ˆì˜ì‘ë‹µì„ ê²½í—˜í•˜ì„¸ìš”!")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì • ë° config ë¡œë“œ
try:
    project_root = get_project_root_dir()
    config = load_config(project_root)
except Exception as e:
    st.error(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    st.stop()

# .env íŒŒì¼ ë¡œë”© (API Key ë“± private ì •ë³´ ì²˜ë¦¬ìš©)
dotenv_path = os.path.join(project_root, ".env")
if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path)
else:
    st.warning(".env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
else: # ì„¸ì…˜ ìƒíƒœê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°, chat_historyë¥¼ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ
    st.session_state.chat_history = st.session_state.get("chat_history", [])
    config["chat_history"] = st.session_state.chat_history

if "docs" not in st.session_state:
    st.session_state.docs = None


# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ìºì‹œ í•¨ìˆ˜
@st.cache_resource
def get_generation_model(model_type: str, model_name: str, use_quantization: bool = False) -> Dict:
    """
    ì§€ì •ëœ ëª¨ë¸ íƒ€ì… ë° ì´ë¦„ì— ë”°ë¼ ìƒì„± ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        model_type (str): ìƒì„± ëª¨ë¸ ì¢…ë¥˜ ('huggingface' ë˜ëŠ” 'openai')
        model_name (str): ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        use_quantization (bool, optional): ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€. ê¸°ë³¸ê°’ì€ False.

    Returns:
        Dict: ë¡œë“œëœ ëª¨ë¸ ì •ë³´ (ì˜ˆ: pipeline, tokenizer ë“± í¬í•¨)
    """
    try:
        config = {
            'generator': {
                'model_type': model_type,
                'model_name': model_name,
                'use_quantization': use_quantization
            }
        }

        if model_type == 'huggingface':
            return load_hf_model(config)
        elif model_type == 'openai':
            return load_openai_model(config)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
    
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        st.stop()


def api_key_verification(embed_model):
    if embed_model.strip().lower() == "openai":
        load_dotenv()
        if not os.environ["OPENAI_API_KEY"]:
            openai_key = st.text_input("ğŸ”‘ OpenAI API Key", type="password")
            os.environ["OPENAI_API_KEY"] = openai_key
            if not openai_key:
                st.warning("OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ API í‚¤ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")


# ì‚¬ì´ë“œë°” êµ¬ì„±
with st.sidebar:
    st.subheader("âš™ï¸ ì„¤ì •")
     # Data ê´€ë ¨ ì„¤ì •
    st.subheader("ğŸ“‚ ë°ì´í„° ì„¤ì •")
    config["data"]["top_k"] = st.slider("ğŸ”¢ ìµœëŒ€ ë¬¸ì„œ ìˆ˜(files)", 1, 100, config["data"]["top_k"])
    config["data"]["file_type"] = st.selectbox("ğŸ“„ íŒŒì¼ ìœ í˜•", ["all", "pdf", "hwp"], index=["all", "pdf", "hwp"].index(config["data"]["file_type"]))
    config["data"]["apply_ocr"] = st.toggle("ğŸ§¾ OCR ì ìš© ì—¬ë¶€", config["data"]["apply_ocr"])
    config["data"]["splitter"] = st.selectbox("âœ‚ï¸ ë¬¸ì„œ ë¶„í•  ë°©ë²•", ["section", "recursive", "token"], index=["section", "recursive", "token"].index(config["data"]["splitter"]))
    config["data"]["chunk_size"] = st.number_input("ğŸ“ Chunk í¬ê¸°", value=config["data"]["chunk_size"], step=100)
    config["data"]["chunk_overlap"] = st.number_input("ğŸ” Chunk ì˜¤ë²„ë©", value=config["data"]["chunk_overlap"], step=10)

    # Embedding ì„¤ì •
    st.subheader("ğŸ§  ì„ë² ë”© ì„¤ì •")
    config["embedding"]["embed_model"] = st.text_input("ğŸ§¬ ì„ë² ë”© ëª¨ë¸", config["embedding"]["embed_model"])
    config["embedding"]["db_type"] = st.selectbox("ğŸ’¾ Vector DB íƒ€ì…", ["faiss", "chroma"], index=["faiss", "chroma"].index(config["embedding"]["db_type"]))

    # api_key í™•ì¸
    api_key_verification(config["embedding"]["embed_model"])

    # Retriever ì„¤ì •
    st.subheader("ğŸ” ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •")
    config["retriever"]["search_type"] = st.selectbox("ğŸ” ê²€ìƒ‰ ë°©ì‹", ["similarity", "hybrid"], index=["similarity", "hybrid"].index(config["retriever"]["search_type"]))
    config["retriever"]["top_k"] = st.slider("ğŸ“„ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜(chunks)", 1, 20, config["retriever"]["top_k"])
    config["retriever"]["rerank"] = st.toggle("ğŸ“Š ë¦¬ë­í¬ ì ìš©", config["retriever"]["rerank"])
    config["retriever"]["rerank_top_k"] = st.slider("ğŸ” ë¦¬ë­í¬ ë¬¸ì„œ ìˆ˜(chunks)", 1, 20, config["retriever"]["rerank_top_k"])

    # Generator ì„¤ì •
    st.subheader("ğŸ” ìƒì„±ì ì„¤ì •")
    config["generator"]["model_type"] = st.selectbox("ğŸ” ìƒì„± ëª¨ë¸ íƒ€ì…", ["huggingface", "openai"], index=["huggingface", "openai"].index(config["generator"]["model_type"]))
    config["generator"]["model_name"] = st.text_input("ğŸ§¬ ìƒì„± ëª¨ë¸", config["generator"]["model_name"])
    config["generator"]["max_length"] = st.number_input("ğŸ”¢ ìµœëŒ€ í† í° ìˆ˜(max_length)", value=config["generator"]["max_length"], step=32)

    # api_key ì¬í™•ì¸
    api_key_verification(config["generator"]["model_type"])

    reset_vector_db = st.button("âš ï¸ Vector DB ì´ˆê¸°í™”")
    
    if config["embedding"]["db_type"] == "faiss":
        faiss_index_name = f"{generate_index_name(config)}"
        vector_db_file = os.path.join(project_root, 'data', f"{faiss_index_name}.faiss")
        metadata_file = os.path.join(project_root, 'data', f"{faiss_index_name}.pkl")
    else:
        chroma_folder_name = f"{generate_index_name(config)}"
        chroma_path = os.path.join(project_root, 'data', chroma_folder_name)


    if reset_vector_db:
        # ì„ íƒëœ ë²¡í„° DB ê²½ë¡œ ì‚­ì œ
        try:
            if config["embedding"]["db_type"] == "faiss":
                if os.path.exists(vector_db_file):
                    os.remove(vector_db_file)
                    os.remove(metadata_file)
                    st.success("FAISS DB ì‚­ì œ ì™„ë£Œ")
                else:
                    st.info("FAISS íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            elif config["embedding"]["db_type"] == "chroma":
                import shutil
                if os.path.exists(chroma_path):
                    shutil.rmtree(chroma_path)
                    st.success("Chroma DB ì‚­ì œ ì™„ë£Œ")
                else:
                    st.info("Chroma í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"Vector DB ì‚­ì œ ì‹¤íŒ¨: {e}")
            
    # ì„¤ì • ë²„íŠ¼
    cols = st.columns([4, 6])
    with cols[0]:
        if st.button("ğŸ”„ ë¦¬ì…‹"):
            st.session_state.chat_history = []
            st.session_state.docs = None
            st.session_state.past_chunks = []
            st.rerun()
    with cols[1]:
        if st.button("ğŸ” ëª¨ë¸ ë¦¬ë¡œë“œ"):
            get_generation_model.clear()
            st.rerun()

# íƒ­ êµ¬ì„±
tab1, tab2 = st.tabs(["ğŸ’¬ ì±—ë´‡", "ğŸ“„ ë¬¸ì„œ ìš”ì•½ ë° ë¶„ì„"])

model_type = config["generator"]["model_type"]
model_name = config["generator"]["model_name"]
use_quantization = config["generator"]["use_quantization"]

with tab1:
    query = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    # ì§ˆë¬¸ ì²˜ë¦¬
    if query:
        if not isinstance(query, str) or query.strip() == "":
            st.warning("ì§ˆë¬¸ì„ ì˜¬ë°”ë¥´ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()

        # ì‚¬ì´ë“œë°” ì„¤ì • ë°˜ì˜ - Vector DB ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if config["data"]["top_k"] == 100:
            if config["embedding"]["db_type"] == "faiss":
                is_save = not os.path.exists(vector_db_file)
            elif config["embedding"]["db_type"] == "chroma":
                is_save = not os.path.exists(chroma_path)
            else:
                is_save = True
        else:
            is_save = True
            
        # ì§ˆë¬¸ ì…ë ¥ì‹œ ì´ì „ ì¶”ì¶œë¬¸ì„œ ê¸°ë¡ ì´ˆê¸°í™”
        if st.session_state.docs is not None:
            st.session_state.docs = None
            
        with st.chat_message("user"):
            st.markdown(query)

        if config.get("chat_history"):  # chat_historyì— ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°
            query_c = f"ì´ì „ ì§ˆë¬¸ ìš”ì•½: {load_chat_history(config)}\nì§ˆë¬¸: {query}"
            config["retriever"]["query"] = query_c
        else:  # chat_historyê°€ ë¹„ì–´ ìˆê±°ë‚˜ ì—†ì„ ê²½ìš°
            config["retriever"]["query"] = query
            pass  # queryëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€

        print(f"ì§ˆë¬¸: {config['retriever']['query']}")

        # ë²¡í„° DBì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        # ë°ì´í„° ì²˜ë¦¬
        # try:
        #     chunks = loader_main(config)
        #     embeddings = generate_embedding(config['embedding']['embed_model'])
            
        #     with st.spinner("ğŸ“‚ ê´€ë ¨ ë¬¸ì„œ ì„ë² ë”© ì¤‘..."):
        #         vector_store = embedding_main(config, chunks, embeddings=embeddings, is_save=is_save) # merged_chunks
        #     with st.spinner("ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
        #         docs = retrieval_main(config, vector_store, chunks) # merged_chunks
        # except Exception as e:
        #     st.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        #     st.stop()
        
        # st.session_state.docs = docs 

        # # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°ëŠ” ë‹¨ í•œë²ˆë§Œ!
        # model_info = get_generation_model(model_type, 
        #                               model_name, 
        #                               use_quantization)

        # # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±, ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        # start_time = time.time()
        # with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
        #     answer = generator_main(docs, config, model_info=model_info) # generator_main í•¨ìˆ˜ì— docsì™€ queryë¥¼ ì „ë‹¬
        # end_time = time.time()
        # elapsed = round(end_time - start_time, 2)

        try:
            with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
                result = rag_pipeline()  # ë‚´ë¶€ì ìœ¼ë¡œ trace ë° printë¡œ ë¡œê·¸ ì¶œë ¥

            # ê²°ê³¼ Streamlitì— ë°˜ì˜
            st.session_state.docs = result["docs"]
            answer = result["answer"]
            elapsed = result["elapsed_time"]

        except Exception as e:
            st.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            st.stop()

        # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
        st.session_state.chat_history.append({"role": "user", "content": query})
        st.session_state.chat_history.append({"role": "ai", "content": answer})

        # ì¶”ë¡  ì‹œê°„ í‘œì‹œ
        with st.chat_message("assistant"):
            st.markdown(f"ğŸ•’ **ì¶”ë¡  ì‹œê°„:** {elapsed}ì´ˆ")
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        config["chat_history"] = st.session_state.chat_history
        # st.rerun()

        # ëœë”ë§ í•œê³„ì : 20ê°œê¹Œì§€ íˆìŠ¤í† ë¦¬ í‘œì‹œ
        MAX_CHAT_HISTORY = 20
        if len(st.session_state.chat_history) > MAX_CHAT_HISTORY:
            st.session_state.chat_history = st.session_state.chat_history[-MAX_CHAT_HISTORY:]

    # ì´ì „ ëŒ€í™” ì¶œë ¥
    for turn in st.session_state.chat_history[::-1]:
        with st.chat_message("user" if turn["role"] == "user" else "assistant"):
            st.markdown(turn["content"])

with tab2:
    st.subheader("ğŸ“„ ë¬¸ì„œ ìš”ì•½ ë° ë¶„ì„")

    docs = st.session_state.get("docs", None)

    if docs is None:
        st.info("â— ë¨¼ì € ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì„¸ìš”.")
    elif isinstance(docs, list) and len(docs) > 0:
        for i, doc in enumerate(docs):
            with st.expander(f"[{i+1}] {doc.metadata.get('ì‚¬ì—…ëª…', 'ì œëª© ì—†ìŒ')}"):
                st.write("ğŸ“„ **ë©”íƒ€ë°ì´í„°**")
                st.json(doc.metadata)
                st.write("ğŸ“ **ë¬¸ì„œ ë‚´ìš©**")
                st.write(doc.page_content)
    elif isinstance(docs, list) and len(docs) == 0:
        st.warning("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info(docs.page_content)