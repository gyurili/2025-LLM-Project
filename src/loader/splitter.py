import re
import numpy as np
import pandas as pd

from collections import defaultdict
from typing import List
from langchain.schema import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter


def clean_text(text: str) -> str:
    """
    ì…ë ¥ ë¬¸ìì—´ì—ì„œ ë¶ˆí•„ìš”í•œ ë¬¸ì ë° ê³µë°±ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

    Args:
        text (str): ì „ì²˜ë¦¬í•  ì›ë³¸ ë¬¸ìì—´

    Returns:
        str: ì •ì œëœ ë¬¸ìì—´
        
    Raises:
        ValueError: ì…ë ¥ì´ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°
    """
    if not isinstance(text, str):
        raise ValueError("âŒ [Type] (splitter.clean_text) ë¬¸ìì—´ì´ ì•„ë‹Œ ì…ë ¥ê°’")

    allowed_pattern = r"[^\uAC00-\uD7A3a-zA-Z0-9\s.,:;!?()\[\]~\-/â€¢â€»ââ–¡ã…‡â—‹â‘ -â‘³IVXLCDM]"
    text = re.sub(allowed_pattern, " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def extract_sections(text: str) -> List[dict]:
    """
    ë‹¤ì–‘í•œ ëª©ì°¨ íŒ¨í„´ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.

    Args:
        text (str): ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸

    Returns:
        List[dict]: ë¶„ë¦¬ëœ ì„¹ì…˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì€ 'title'ê³¼ 'content' í¬í•¨)
    """
    section_pattern = re.compile(
        r"""
        ^[ \t]*
        (
            (?:\d+(?:\.\d+)*[.)]?) |
            (?:[ê°€-í£]{1}[.)]?) |
            (?:\[\d+\]) |
            (?:\[\s*ë¶™ì„\s*\d+\s*\]) |
            (?:[â‘ -â‘³]) |
            (?:[â—‹â€¢â€»ââ–¡ã…‡]) |
            (?:[IVXLCDM]{1,7}\.?)
        )
        [ \t]+
        ([^\n]{2,50})
        """,
        re.MULTILINE | re.VERBOSE
    )

    matches = list(section_pattern.finditer(text))

    chunks = []
    for i in range(len(matches)):
        start = matches[i].start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        title = matches[i].group().strip()
        content = text[start:end].strip()
        chunks.append({"title": title, "content": content})
    return chunks


def merge_short_chunks(chunks: List[dict], min_length: int = 500) -> List[dict]:
    """
    ê¸¸ì´ê°€ min_length ë¯¸ë§Œì¸ ì²­í¬ë“¤ì„ ì¸ì ‘í•œ ì²­í¬ì— ë³‘í•©í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        chunks (List[dict]): ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        min_length (int, optional): ë³‘í•© ê¸°ì¤€ì´ ë˜ëŠ” ìµœì†Œ ê¸¸ì´ (ê¸°ë³¸ê°’: 500)

    Returns:
        List[dict]: ë³‘í•©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    merged = []
    buffer = ""
    for chunk in chunks:
        if len(chunk["content"]) < min_length:
            buffer += " " + chunk["content"]
        else:
            if buffer:
                chunk["content"] = buffer.strip() + " " + chunk["content"]
                buffer = ""
            merged.append(chunk)
    return merged


def refine_chunks_with_length_control(
    chunks: List[dict],
    max_length: int = 1000,
    overlap: int = 250
) -> List[dict]:
    """
    ê° ì²­í¬ì˜ ê¸¸ì´ë¥¼ ì œí•œí•˜ë©´ì„œ ê²¹ì¹˜ëŠ” ì˜ì—­ì„ í¬í•¨í•´ ì¶”ê°€ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        chunks (List[dict]): ë³‘í•©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        max_length (int, optional): ìµœëŒ€ ì²­í¬ ê¸¸ì´ (ê¸°ë³¸ê°’: 1000)
        overlap (int, optional): ì²­í¬ ê°„ ì¤‘ì²© ê¸¸ì´ (ê¸°ë³¸ê°’: 250)

    Returns:
        List[dict]: ê¸¸ì´ ì œí•œ ë° ì¤‘ì²© ì²˜ë¦¬ê°€ ì ìš©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    refined = []
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=max_length, chunk_overlap=overlap
    )

    for chunk in chunks:
        split_texts = splitter.split_text(chunk["content"])
        for i, split_text in enumerate(split_texts):
            refined.append(
                {
                    "title": chunk["title"],
                    "content": split_text,
                    "sub_chunk_idx": i,
                }
            )
    return refined


def data_chunking(
    df: pd.DataFrame,
    splitter_type: str = "section",
    size: int = 1000,
    overlap: int = 250,
) -> List[Document]:
    """
    ë°ì´í„°í”„ë ˆì„ì˜ ê° rowë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³ , langchain Documentë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        df (pd.DataFrame): 'full_text' ì»¬ëŸ¼ì„ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„
        splitter_type (str, optional): ë¶„í•  ë°©ì‹ ('recursive', 'token', 'section')
        size (int, optional): ì²­í¬ ìµœëŒ€ í¬ê¸° (ê¸°ë³¸ê°’: 1000)
        overlap (int, optional): ì²­í¬ ê°„ ì¤‘ì²© ê¸¸ì´ (ê¸°ë³¸ê°’: 250)

    Returns:
        List[Document]: ì²­í¬ ë‹¨ìœ„ë¡œ ë³€í™˜ëœ langchain Document ë¦¬ìŠ¤íŠ¸

    Raises:
        ValueError: í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ splitter_typeì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš°
        RuntimeError: ì²­í¬ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ ì‹œ
    """
    if splitter_type == "recursive":
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=size, chunk_overlap=overlap
        )
    elif splitter_type == "token":
        splitter = TokenTextSplitter(chunk_size=size, chunk_overlap=overlap)
    elif splitter_type == "section":
        splitter = None
    else:
        raise ValueError(
            f"âŒ [Value] (splitter.data_chunking.splitter_type) ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„í•  ë°©ì‹: {splitter_type}"
        )

    all_chunks = []
    for _, row in df.iterrows():
        text = row.get("full_text", "")
        if isinstance(text, str) and text.strip():
            try:
                text = clean_text(text)
                if splitter_type == "section":
                    sections = extract_sections(text)
                    if not sections:
                        print(f"âš ï¸ [Skip] ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ì²­í¬ ì—†ìŒ: {row.get('íŒŒì¼ëª…')}")    
                    merged = merge_short_chunks(sections)
                    chunks = refine_chunks_with_length_control(
                        merged, max_length=size, overlap=overlap
                    )
                    if not chunks:
                        print(f"âš ï¸ [Skip] ì²­í¬ 0ê°œ ìƒì„±ë¨: {row.get('íŒŒì¼ëª…')}")
                else:
                    chunks = splitter.split_text(text)

                for i, chunk in enumerate(chunks):
                    doc = Document(
                        page_content=chunk["content"]
                        if isinstance(chunk, dict)
                        else chunk,
                        metadata={
                            "ì‚¬ì—…ëª…": row.get("ì‚¬ì—…ëª…", ""),
                            "ë°œì£¼ ê¸°ê´€": row.get("ë°œì£¼ ê¸°ê´€", ""),
                            "íŒŒì¼ëª…": row.get("íŒŒì¼ëª…", ""),
                            "chunk_idx": i,
                            "chunk_title": chunk.get("title", "")
                            if isinstance(chunk, dict)
                            else "",
                        },
                    )
                    all_chunks.append(doc)
            except Exception as e:
                raise RuntimeError(
                    f"âŒ [Runtime] (splitter.data_chunking) ì²­í¬ ìƒì„± ì˜¤ë¥˜ ({row.get('íŒŒì¼ëª…')}): {e}"
                )
        else:
            print(f"âš ï¸ [Skip] full_text ë¹„ì–´ìˆì–´ ì²­í¬ ê±´ë„ˆëœ¸: {row.get('íŒŒì¼ëª…')}")
            continue  # ê±´ë„ˆë›°ê¸°

    return all_chunks


def inspect_sample_chunks(
    chunks: List[Document], file_name: str, verbose: bool = False
) -> None:
    """
    íŠ¹ì • íŒŒì¼ì— í•´ë‹¹í•˜ëŠ” ì²­í¬ë“¤ ì¤‘ ì£¼ìš” ì²­í¬(ì²«, ì¤‘ê°„, ë§ˆì§€ë§‰, ìµœëŒ€/ìµœì†Œ ê¸¸ì´)ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        chunks (List[Document]): ì „ì²´ Document ì²­í¬ ë¦¬ìŠ¤íŠ¸
        file_name (str): í™•ì¸í•  ëŒ€ìƒ íŒŒì¼ëª…
        verbose (bool, optional): ì¶œë ¥ ì—¬ë¶€

    Returns:
        None
    """
    if not verbose:
        return

    file_chunks = [doc for doc in chunks if doc.metadata.get("íŒŒì¼ëª…") == file_name]
    if not file_chunks:
        print(f"âŒ [Data] (splitter.inspect_sample_chunks) ì²­í¬ ì—†ìŒ: {file_name}")
        return

    lengths = [len(doc.page_content) for doc in file_chunks]
    idx_max = lengths.index(max(lengths))
    idx_min = lengths.index(min(lengths))

    selected = {
        "ì²« ì²­í¬": file_chunks[0],
        "ì¤‘ê°„ ì²­í¬": file_chunks[len(file_chunks) // 2],
        "ë§ˆì§€ë§‰ ì²­í¬": file_chunks[-1],
        "ê°€ì¥ ê¸´ ì²­í¬": file_chunks[idx_max],
        "ê°€ì¥ ì§§ì€ ì²­í¬": file_chunks[idx_min],
    }

    for label, doc in selected.items():
        print(f"\nâ–¶ {label}")
        print(f"  - ê¸¸ì´: {len(doc.page_content)}")
        print("  - ë‚´ìš©:")
        preview = doc.page_content[:300]
        if len(doc.page_content) > 300:
            preview += "..."
        print(f"    {preview}")


def summarize_chunk_quality(
    chunks: List[Document], verbose: bool = False
) -> None:
    """
    íŒŒì¼ë³„ë¡œ ì²­í¬ ìˆ˜, í‰ê· /ìµœì†Œ/ìµœëŒ€ ê¸¸ì´, 500ì ë¯¸ë§Œ ë¹„ìœ¨ ë“±ì˜ ì²­í¬ í’ˆì§ˆ í†µê³„ë¥¼ ìš”ì•½ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        chunks (List[Document]): ì „ì²´ Document ì²­í¬ ë¦¬ìŠ¤íŠ¸
        verbose (bool, optional): ìƒ˜í”Œ ì²­í¬ ì¶œë ¥ ì—¬ë¶€

    Returns:
        None
    """
    if not verbose:
        return

    summary = defaultdict(list)
    for doc in chunks:
        file_name = doc.metadata.get("íŒŒì¼ëª…", "Unknown")
        length = len(doc.page_content)
        summary[file_name].append(length)

    results = []
    for fname, lengths in summary.items():
        arr = np.array(lengths)
        results.append(
            {
                "íŒŒì¼ëª…": fname,
                "ì²­í¬ìˆ˜": len(arr),
                "í‰ê· ê¸¸ì´": np.mean(arr),
                "ìµœì†Œê¸¸ì´": np.min(arr),
                "ìµœëŒ€ê¸¸ì´": np.max(arr),
                "500ìë¯¸ë§Œë¹„ìœ¨": np.sum(arr < 500) / len(arr) * 100,
            }
        )

    results.sort(key=lambda x: x["500ìë¯¸ë§Œë¹„ìœ¨"], reverse=True)

    print("\nğŸ“Œ ì²­í¬ í’ˆì§ˆ ìš”ì•½")
    for res in results:
        print("=" * 60)
        print(f"ğŸ“„ íŒŒì¼ëª…: {res['íŒŒì¼ëª…']}")
        print(f"  - ì²­í¬ ìˆ˜         : {res['ì²­í¬ìˆ˜']}")
        print(f"  - í‰ê·  ê¸¸ì´       : {res['í‰ê· ê¸¸ì´']:.2f}")
        print(f"  - ìµœì†Œ ê¸¸ì´       : {res['ìµœì†Œê¸¸ì´']}")
        print(f"  - ìµœëŒ€ ê¸¸ì´       : {res['ìµœëŒ€ê¸¸ì´']}")
        print(f"  - 500ì ë¯¸ë§Œ ë¹„ìœ¨ : {res['500ìë¯¸ë§Œë¹„ìœ¨']:.2f}%")
        inspect_sample_chunks(chunks, res['íŒŒì¼ëª…'], verbose=True)