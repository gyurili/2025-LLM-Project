import os
import fitz
import easyocr
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
from pathlib import Path
from typing import List
from langchain.schema import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain_teddynote.document_loaders import HWPLoader
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_openai import OpenAIEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from src.utils.path import get_project_root_dir

# EasyOCR Reader ê°ì²´ ìƒì„± (GPU ì‚¬ìš©)
# í•œê¸€(ko) + ì˜ì–´(en)ë¥¼ ì¸ì‹í•˜ë©°, ëª¨ë¸ì€ í•œ ë²ˆë§Œ ë¡œë“œë¨
reader = easyocr.Reader(['ko', 'en'], gpu=True)


def safe_ocr(img_array: np.ndarray, ocr_reader: easyocr.Reader) -> str:
    """
    ì´ë¯¸ì§€ ë°°ì—´ì„ ì…ë ¥ë°›ì•„ EasyOCRë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        img_array (np.ndarray): OCRì„ ìˆ˜í–‰í•  ì´ë¯¸ì§€ ë°°ì—´
        ocr_reader (easyocr.Reader): ì´ˆê¸°í™”ëœ EasyOCR ë¦¬ë” ì¸ìŠ¤í„´ìŠ¤

    Returns:
        str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¬¸ìì—´
    """
    try:
        result = ocr_reader.readtext(img_array, detail=0)
        if not isinstance(result, list):
            return ""
        return "\n".join(result)
    except Exception as e:
        raise RuntimeError(f"âŒ [OCR] (data_loader.safe_ocr) OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}")


def extract_text_from_pdf(pdf_path: Path, apply_ocr: bool = True) -> str:
    """
    PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ë° OCR í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        pdf_path (Path): ì²˜ë¦¬í•  PDF íŒŒì¼ ê²½ë¡œ
        apply_ocr (bool): OCR ìˆ˜í–‰ ì—¬ë¶€

    Returns:
        str: ì „ì²´ í˜ì´ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    if not pdf_path.exists():
        raise FileNotFoundError(f"âŒ(data_loader.extract_text_from_pdf.pdf_path) PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
    full_text = ""
    with fitz.open(pdf_path) as doc:
        for page_num, page in enumerate(tqdm(doc, desc=f"{pdf_path.name}")):
            page_text = page.get_text()
            full_text += page_text

            if apply_ocr:
                pix = page.get_pixmap(dpi=300)
                img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                ocr_text = safe_ocr(np.array(img), reader)
                if ocr_text.strip():
                    full_text += f"\n[OCR p.{page_num + 1}]\n{ocr_text}"
    return full_text


def retrieve_top_documents_from_metadata(query, csv_path, top_k=5, verbose=False):
    """
    ì‚¬ìš©ì ì§ˆë¬¸(query)ê³¼ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°(csv)ì— ê¸°ë°˜í•˜ì—¬ 
    ê°€ì¥ ìœ ì‚¬í•œ top_kê°œì˜ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        query (str): ì‚¬ìš©ì ì§ˆë¬¸
        csv_path (str): CSV íŒŒì¼ ê²½ë¡œ
        top_k (int): ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’ 5)

    Returns:
        pd.DataFrame: ìƒìœ„ top_k ë¬¸ì„œ ì •ë³´ + ìœ ì‚¬ë„ ì ìˆ˜
    """
    try:
        # 0. ëª¨ë¸ ë¡œë“œ
        sbert_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    except Exception as e:
        raise RuntimeError(f"âŒ SBERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")

    # 1. CSV íŒŒì¼ ë¡œë“œ
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
    
    try:
        df = pd.read_csv(csv_path)
    except Exception as e:
        raise ValueError(f"âŒ CSV íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
    
    # 2. í•„ìš”í•œ ì—´ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    required_columns = ["ì‚¬ì—…ëª…", "ë°œì£¼ ê¸°ê´€", "ì‚¬ì—… ìš”ì•½", "íŒŒì¼ëª…"]
    for col in required_columns:
        if col not in df.columns:
            raise KeyError(f"âŒ '{col}' ì—´ì´ CSVì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # 3. ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„±
    def make_embedding_text(row):
        return f"{row['ì‚¬ì—…ëª…']} {row['ë°œì£¼ ê¸°ê´€']} {row['ì‚¬ì—… ìš”ì•½']}"
    
    try:
        df["ì„ë² ë”©í…ìŠ¤íŠ¸"] = df.apply(make_embedding_text, axis=1)
    except Exception as e:
        raise RuntimeError(f"âŒ ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

    # 4. ë¬¸ì„œ ì„ë² ë”© ìƒì„±
    try:
        doc_embeddings = sbert_model.encode(df["ì„ë² ë”©í…ìŠ¤íŠ¸"].tolist(), convert_to_tensor=True)
    except Exception as e:
        raise RuntimeError(f"âŒ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")

    # 5. ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
    try:
        query_embedding = sbert_model.encode(query, convert_to_tensor=True)
    except Exception as e:
        raise RuntimeError(f"âŒ ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")

    # 6. ìœ ì‚¬ë„ ê³„ì‚°
    try:
        similarities = cosine_similarity(
            query_embedding.cpu().numpy().reshape(1, -1), 
            doc_embeddings.cpu().numpy()
        )[0]
    except Exception as e:
        raise RuntimeError(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")

    # 7. ìƒìœ„ top_k ì¸ë±ìŠ¤ ì¶”ì¶œ
    top_k_indices = np.argsort(similarities)[::-1][:top_k]

    # 8. ê²°ê³¼ DataFrame ë°˜í™˜
    try:
        top_docs = df.iloc[top_k_indices].copy()
        top_docs["ìœ ì‚¬ë„"] = similarities[top_k_indices]
    except Exception as e:
        raise RuntimeError(f"âŒ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    if verbose:
        print("ğŸ“„ Top ë¬¸ì„œ ëª©ë¡:")
        print(top_docs[["íŒŒì¼ëª…", "ìœ ì‚¬ë„"]])

    return top_docs

from src.utils.path import get_project_root_dir

def data_process(df: pd.DataFrame, apply_ocr: bool = True, file_type: str = "all") -> pd.DataFrame:
    """
    HWP ë˜ëŠ” PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  full_text ì»¬ëŸ¼ì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        df (pd.DataFrame): íŒŒì¼ ëª©ë¡ì„ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„
        apply_ocr (bool): PDF OCR ì—¬ë¶€
        file_type (str): 'hwp', 'pdf', 'all' ì¤‘ í•˜ë‚˜

    Returns:
        pd.DataFrame: í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ DataFrame
    """
    base_dir = get_project_root_dir()
    file_root = os.path.join(base_dir, "data", "files")

    if file_type in ["hwp", "pdf"]:
        mask = df['íŒŒì¼ëª…'].str.lower().str.endswith(f".{file_type}")
        filtered_df = df[mask].copy()
    elif file_type == "all":
        filtered_df = df.copy()

    filtered_df['full_text'] = None

    for file_name in filtered_df['íŒŒì¼ëª…']:
        file_path = os.path.join(file_root, file_name)
        try:
            if not os.path.exists(file_path):
                 raise FileNotFoundError(f"âŒ [FileNotFound] (data_loader.data_process.path) íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")

            if file_name.lower().endswith(".hwp") and file_type in ["hwp", "all"]:
                loader = HWPLoader(file_path)
                docs = loader.load()
                if docs and isinstance(docs[0].page_content, str):
                    filtered_df.loc[filtered_df['íŒŒì¼ëª…'] == file_name, 'full_text'] = docs[0].page_content
                else:
                    print(f"âš ï¸ [Warning] (data_loader.data_process.hwp) HWP íŒŒì¼ ë¬´ì‹œë¨ (ë‚´ìš© ì—†ìŒ): {file_name}")

            elif file_name.lower().endswith(".pdf") and file_type in ["pdf", "all"]:
                text = extract_text_from_pdf(Path(file_path), apply_ocr=apply_ocr)
                filtered_df.loc[filtered_df['íŒŒì¼ëª…'] == file_name, 'full_text'] = text

            else:
                print(f"âš ï¸ [Warning] (data_loader.data_process) ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_name}")

        except Exception as e:
            raise RuntimeError(f"âŒ [Runtime] (data_loader.data_process) íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_name}): {e}")  

    return filtered_df.reset_index(drop=True)
