import os
from typing import List, Union

import faiss
from dotenv import load_dotenv
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_openai import OpenAIEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings

def generate_embedding(embed_model_name: str) -> Union[OpenAIEmbeddings, HuggingFaceEmbeddings]:
    """
    ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    Args:
        embed_model_name (str): 'openai' ë˜ëŠ” huggingface ëª¨ë¸ ì´ë¦„

    Returns:
        ì„ë² ë”© ê°ì²´

    Raises:
        ValueError: ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
    """
    try:
        if embed_model_name == "openai":
            load_dotenv()
            return OpenAIEmbeddings()
        else:
            return HuggingFaceEmbeddings(model_name=embed_model_name)
    except Exception as e:
        raise ValueError(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

def generate_vector_db(all_chunks: List[Document], embed_model_name: str) -> Union[OpenAIEmbeddings, HuggingFaceEmbeddings]:
    """
    FAISS ê¸°ë°˜ ë²¡í„° DBë¥¼ ìƒì„±í•˜ê³  ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        all_chunks (List[Document]): ì²­í¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        embed_model_name (str): ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„

    Returns:
        ì„ë² ë”© ê°ì²´

    Raises:
        ValueError: ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
    """
    embeddings = generate_embedding(embed_model_name)
    try:
        dimension = len(embeddings.embed_query("hello world"))
    except Exception as e:
        raise ValueError(f"ì„ë² ë”© ì°¨ì› ê³„ì‚° ì‹¤íŒ¨: {e}")

    vector_store = FAISS(
        embedding_function=embeddings,
        index=faiss.IndexFlatL2(dimension),
        docstore=InMemoryDocstore(),
        index_to_docstore_id={}
    )
    vector_store.add_documents(all_chunks)

    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_path = os.path.join(base_dir, "data")
    
    vector_store.save_local(folder_path=output_path, index_name="hwp_faiss_index")
    return embeddings

def load_vector_db(path: str, embed_model_name: str) -> FAISS:
    """
    ë¡œì»¬ì—ì„œ ì €ì¥ëœ FAISS ë²¡í„° DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        path (str): ë²¡í„° DBê°€ ì €ì¥ëœ ë£¨íŠ¸ ê²½ë¡œ (ì˜ˆ: "data")
        embed_model_name (str): ì„ë² ë”© ëª¨ë¸ ì´ë¦„

    Returns:
        FAISS: ë¡œë“œëœ ë²¡í„° DB ê°ì²´

    Raises:
        FileNotFoundError: ê²½ë¡œê°€ ì—†ì„ ê²½ìš°
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"ë²¡í„° DB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")

    embeddings = generate_embedding(embed_model_name)

    return FAISS.load_local(
        folder_path=path,
        index_name="hwp_faiss_index",
        embeddings=embeddings,
        allow_dangerous_deserialization=True,
    )


'''
from src.data_load import (data_load, data_process, data_chunking)

if __name__ == "__main__":
    try:
        df = data_load("data/data_list.csv")
        df = data_process(df)
        all_chunks = data_chunking(df)

        print("âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ!")

        embeddings = generate_vector_db(all_chunks, "open_ai")
        print("âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ!")

        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        vector_db_path = os.path.join(base_dir, "data")

        vector_store = load_vector_db(vector_db_path, "open_ai")
        print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ!")

        docs = vector_store.similarity_search("ë°°ë“œë¯¼í„´ì¥ ë° íƒêµ¬ì¥ ì˜ˆì•½ë°©ë²•", k=8)
        for i, doc in enumerate(docs, start=1):
            print(f"\nğŸ“„ ìœ ì‚¬ ë¬¸ì„œ {i}:\n{doc.page_content}")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
'''