from typing import List, Optional, Literal
from collections import defaultdict
from langchain.vectorstores.base import VectorStore
from langchain.schema import Document
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from sklearn.metrics.pairwise import cosine_similarity
from src.embedding.vector_db import generate_embedding
from dotenv import load_dotenv
load_dotenv()

import re
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity
from typing import List
from langchain.schema import Document

def compute_keyword_overlap_score(query: str, chunk: str) -> float:
    query_tokens = set(re.findall(r"\w+", query.lower()))
    chunk_tokens = set(re.findall(r"\w+", chunk.lower()))
    if not query_tokens or not chunk_tokens:
        return 0.0
    overlap = query_tokens.intersection(chunk_tokens)
    return len(overlap) / len(query_tokens)

def rerank_documents(
    query: str,
    docs: List[Document],
    embed_model,
    rerank_top_k: int,
    min_chunks: int,
    verbose: bool = False
) -> List[Document]:
    try:
        query_vec = [embed_model.embed_query(query)]
        doc_vecs = [embed_model.embed_documents([doc.page_content])[0] for doc in docs]
    except Exception as e:
        raise RuntimeError(f"âŒ [Runtime] (retrieval.rerank_documents) ì„ë² ë”© ì‹¤íŒ¨: {e}")

    cosine_scores = cosine_similarity(query_vec, doc_vecs)[0]

    hybrid_scores = []
    for doc, cos_sim in zip(docs, cosine_scores):
        keyword_score = compute_keyword_overlap_score(query, doc.page_content)
        hybrid = 0.7 * cos_sim + 0.3 * keyword_score
        hybrid_scores.append((doc, hybrid))

    # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
    doc_groups = defaultdict(list)
    for doc, hybrid_score in hybrid_scores:
        fname = doc.metadata.get("íŒŒì¼ëª…")
        doc_groups[fname].append((doc, hybrid_score))

    if verbose:
        print("\nğŸ“Œ [ë””ë²„ê¹…] ë¬¸ì„œ ê·¸ë£¹ë³„ ìœ ì‚¬ë„ (cosine + keyword + hybrid):")
        for fname, group in doc_groups.items():
            print(f"\nğŸ“ ë¬¸ì„œ: {fname}")
            for doc, hybrid_score in group:
                idx = docs.index(doc)
                cos_sim = cosine_scores[idx]
                keyword_score = compute_keyword_overlap_score(query, doc.page_content)
                print(f"  - chunk_idx: {doc.metadata.get('chunk_idx')}, "
                    f"cosine: {cos_sim:.4f}, keyword: {keyword_score:.4f}, hybrid: {hybrid_score:.4f}")

    # ë¬¸ì„œë³„ í‰ê·  hybrid ìœ ì‚¬ë„ ê³„ì‚°
    doc_scores = []
    for fname, group in doc_groups.items():
        avg_score = sum(score for _, score in group) / len(group)
        doc_scores.append((fname, avg_score))

    if verbose:
        print("\nğŸ“Œ [ë””ë²„ê¹…] ë¬¸ì„œë³„ í‰ê·  hybrid ìœ ì‚¬ë„:")
        for rank, (fname, score) in enumerate(sorted(doc_scores, key=lambda x: x[1], reverse=True), 1):
            print(f"  {rank}. {fname} (í‰ê·  hybrid ìœ ì‚¬ë„: {score:.4f})")

    # ìƒìœ„ ë¬¸ì„œ ì„ íƒ
    top_doc_fnames = set(fname for fname, _ in sorted(doc_scores, key=lambda x: x[1], reverse=True)[:rerank_top_k])

    if verbose:
        print(f"\nâœ… ì„ íƒëœ ìƒìœ„ {rerank_top_k}ê°œ ë¬¸ì„œ:")
        for fname in top_doc_fnames:
            print(f"  - {fname}")

    # ê° ë¬¸ì„œì—ì„œ min_chunks ë§Œí¼ ì²­í¬ ì„ íƒ
    selected_docs = []
    for fname in top_doc_fnames:
        group = sorted(doc_groups[fname], key=lambda x: x[1], reverse=True)
        selected = group[:min_chunks]
        selected_docs.extend([doc for doc, _ in selected])

        if verbose:
            print(f"\nğŸ“„ ë¬¸ì„œ {fname}ì˜ ì„ íƒëœ ìƒìœ„ {min_chunks}ê°œ ì²­í¬:")
            for doc, score in selected:
                print(f"  - chunk_idx: {doc.metadata.get('chunk_idx')}, hybrid ìœ ì‚¬ë„: {score:.4f}")

    return selected_docs


def retrieve_documents(
    query: str,
    vector_store: VectorStore,
    top_k: int,
    search_type: Literal["similarity", "hybrid"],
    chunks: Optional[List[Document]],
    embed_model_name: str,
    rerank: bool,
    rerank_top_k: int,
    min_chunks: int,
    verbose: bool = False
) -> List[Document]:
    """
    TODO:
    - ë¦¬íŠ¸ë¦¬ë²„ëŠ” ë²¡í„°DBì—ì„œ ê²€ìƒ‰í•´ ì²­í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë°, ë¬¸ì„œë§ˆë‹¤ ë¦¬íŠ¸ë¦¬ë²„ê°€ ì ìš©ë˜ê²Œ ë§Œë“¤ì–´ ìµœì†Œ ì²­í¬ ìˆ˜ë¥¼ ë³´ì¥í•´ì•¼ í•  ê²ƒ ê°™ë‹¤.

    ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ëŒ€í•´ similarity ë˜ëŠ” hybrid ê²€ìƒ‰ ë°©ì‹ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ë¦¬ë­í¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ë„ ë¬¸ì„œë§ˆë‹¤ ìµœì†Œ ì²­í¬ ìˆ˜ë¥¼ í™•ë³´í•˜ëŠ” í›„ì²˜ë¦¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

    Args:
        query (str): ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬
        vector_store (VectorStore): ë²¡í„° ì €ì¥ì†Œ ì¸ìŠ¤í„´ìŠ¤
        top_k (int): ê²€ìƒ‰í•  ìµœëŒ€ ë¬¸ì„œ ê°œìˆ˜
        search_type (Literal["similarity", "hybrid"]): ê²€ìƒ‰ ë°©ì‹
        chunks (Optional[List[Document]]): hybrid ê²€ìƒ‰ì„ ìœ„í•œ ì „ì²´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        embed_model_name (str): ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        rerank (bool): re-ranking ì ìš© ì—¬ë¶€
        min_chunks (int): ë¬¸ì„œë§ˆë‹¤ ë³´ì¥ë˜ëŠ” ìµœì†Œ ì²­í¬ ìˆ˜
        verbose (bool): ë””ë²„ê·¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        List[Document]: ê²€ìƒ‰ ë˜ëŠ” ì¬ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

    Raises:
        RuntimeError: retriever ìƒì„±ì— ì‹¤íŒ¨í•  ê²½ìš°
        ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ì‹ ë˜ëŠ” chunks ë¯¸ì œê³µ ì‹œ
    """
    
    try:
        embed_model = generate_embedding(embed_model_name=embed_model_name)
    except Exception as e:
        raise RuntimeError(f"âŒ [Runtime] (retrieval.retrieve_documents.embed_model) ì„ë² ë”© ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        
    if search_type == "similarity":
        try:
            docs = vector_store.similarity_search(query, k=top_k)
        except Exception as e:
            raise RuntimeError(f"âŒ similarity_search ì‹¤íŒ¨: {e}")
        
    elif search_type == "hybrid":
        if chunks is None:
            raise ValueError("âŒ [Value] (retrieval.retrieve_documents.chunks) hybrid ê²€ìƒ‰ì„ ìœ„í•´ chunksê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        try:
            vector_retriever = vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": top_k}
            )
        except Exception as e:
            raise RuntimeError(f"âŒ [Runtime] (retrieval.retrieve_documents.vector_retriever) FAISS retriever ìƒì„± ì‹¤íŒ¨: {e}")
    
        try:
            bm25_retriever = BM25Retriever.from_documents(chunks)
            bm25_retriever.k = top_k
        except Exception as e:
            raise RuntimeError(f"âŒ [Runtime] (retrieval.retrieve_documents.bm25_retriever) BM25 retriever ìƒì„± ì‹¤íŒ¨: {e}")
        
        hybrid_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, vector_retriever],
            weights=[0.5, 0.5]
        )
        docs = hybrid_retriever.invoke(query)
        
        seen_pairs = set()
        unique_docs = []
        for doc in docs:
            identifier = (doc.metadata.get("íŒŒì¼ëª…"), doc.page_content.strip())
            if identifier not in seen_pairs:
                unique_docs.append(doc)
                seen_pairs.add(identifier)
        docs = unique_docs[:top_k]
        
    else:
        raise ValueError(f"âŒ [Value] (retrieval.retrieve_documents.search_type) ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ì‹ì…ë‹ˆë‹¤: {search_type}")

    if rerank:
        docs = rerank_documents(query, docs, embed_model, rerank_top_k, min_chunks, verbose)

    return docs