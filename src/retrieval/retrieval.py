from dotenv import load_dotenv
from typing import List, Optional, Literal

from langsmith import traceable
from langchain.vectorstores.base import VectorStore
from langchain.schema import Document
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from sentence_transformers import CrossEncoder

from src.generator.generator_main import load_chat_history
from src.embedding.vector_db import generate_embedding

load_dotenv()


@traceable(name="rerank_documents")
def rerank_documents(
    query: str,
    docs: List[Document],
    rerank_top_k: int,
    verbose: bool
) -> List[Document]:
    """
    ê²€ìƒ‰ì–´ì™€ ë¬¸ì„œ ê°„ CrossEncoder ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì¬ì •ë ¬í•˜ì—¬ ìƒìœ„ Nê°œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        query (str): ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬
        docs (List[Document]): ê²€ìƒ‰ìœ¼ë¡œ ì¶”ì¶œëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        rerank_top_k (int): ìµœì¢… ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
        verbose (bool): ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        List[Document]: ì¬ì •ë ¬ëœ ìƒìœ„ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    if verbose:
        print("\nğŸ“Œ ê¸°ì¡´ ë¬¸ì„œ ìˆœì„œ:")
        for i, doc in enumerate(docs, 1):
            print(f"  {i}. íŒŒì¼ëª…: {doc.metadata.get('íŒŒì¼ëª…')}, ì²­í¬: {doc.metadata.get('chunk_idx')}")

    model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    pairs = [(query, doc.page_content) for doc in docs]
    scores = model.predict(pairs)

    doc_scores = [(doc, score) for doc, score in zip(docs, scores)]
    doc_scores.sort(key=lambda x: x[1], reverse=True)

    if verbose:
        print("\nğŸ“Œ re-rank ì ìš© í›„ ë¬¸ì„œ ìˆœì„œ:")
        for i, (doc, score) in enumerate(doc_scores, 1):
            print(f"  {i}. íŒŒì¼ëª…: {doc.metadata.get('íŒŒì¼ëª…')}, ì²­í¬: {doc.metadata.get('chunk_idx')}, ì ìˆ˜: {score:.4f}")
    else:
        print("\nğŸ“Œ ìµœì¢… ë¬¸ì„œ ìˆœì„œ(ìƒìœ„ 5ê°œ):")
        for i, (doc, score) in enumerate(doc_scores[:5], 1):
            print(f"  {i}. íŒŒì¼ëª…: {doc.metadata.get('íŒŒì¼ëª…')}, ì²­í¬: {doc.metadata.get('chunk_idx')}, ì ìˆ˜: {score:.4f}")

    return [doc for doc, _ in doc_scores[:rerank_top_k]]


@traceable(name="retrieve_documents")
def retrieve_documents(
    query: str,
    vector_store: VectorStore,
    top_k: int,
    search_type: Literal["similarity", "hybrid"],
    chunks: Optional[List[Document]],
    embed_model: str,
    rerank: bool,
    rerank_top_k: int,
    verbose: bool,
    config
) -> List[Document]:
    """
    ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ëŒ€í•´ similarity ë˜ëŠ” hybrid ê²€ìƒ‰ ë°©ì‹ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        query (str): ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬
        vector_store (VectorStore): ë²¡í„° ì €ì¥ì†Œ ì¸ìŠ¤í„´ìŠ¤
        top_k (int): ê²€ìƒ‰í•  ìµœëŒ€ ë¬¸ì„œ ê°œìˆ˜
        search_type (Literal["similarity", "hybrid"]): ê²€ìƒ‰ ë°©ì‹
        chunks (Optional[List[Document]]): hybrid ê²€ìƒ‰ì„ ìœ„í•œ ì „ì²´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        embed_model (str): ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        rerank (bool): re-ranking ì ìš© ì—¬ë¶€
        rerank_top_k (int): re-ranking ì‹œ ìµœì¢… ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
        verbose (bool): ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        List[Document]: ê²€ìƒ‰ ë˜ëŠ” ì¬ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

    Raises:
        RuntimeError: retriever ìƒì„±ì— ì‹¤íŒ¨í•  ê²½ìš°
        ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ì‹ ë˜ëŠ” chunks ë¯¸ì œê³µ ì‹œ
    """
    try:
        embed_model = generate_embedding(embed_model)
    except Exception as e:
        raise RuntimeError(f"âŒ [Runtime] (retrieval.retrieve_documents.embed_model) ì„ë² ë”© ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
        # ê³¼ê±° ì§ˆì˜ì‘ë‹µ ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê°€
    chat_history = load_chat_history(config)
    query = f"ë§¥ë½: {chat_history}\n ì§ˆë¬¸:{query}"
    
    if search_type == "similarity":
        docs = vector_store.similarity_search(query, k=top_k)
        if rerank:
            docs = rerank_documents(query, docs, rerank_top_k, verbose)

    elif search_type == "hybrid":
        if chunks is None:
            raise ValueError("âŒ [Value] (retrieval.retrieve_documents.chunks) hybrid ê²€ìƒ‰ì„ ìœ„í•´ chunksê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        try:
            vector_retriever = vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": top_k}
            )
        except Exception as e:
            raise RuntimeError(f"âŒ [Runtime] (retrieval.retrieve_documents.vector_retriever) FAISS retriever ìƒì„± ì‹¤íŒ¨: {e}")

        try:
            bm25_retriever = BM25Retriever.from_documents(chunks)
            bm25_retriever.k = top_k
        except Exception as e:
            raise RuntimeError(f"âŒ [Runtime] (retrieval.retrieve_documents.bm25_retriever) BM25 retriever ìƒì„± ì‹¤íŒ¨: {e}")

        hybrid_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, vector_retriever],
            weights=[0.4, 0.6]
        )
        docs = hybrid_retriever.invoke(query)

        seen_pairs = set()
        unique_docs = []
        for doc in docs:
            identifier = (doc.metadata.get("íŒŒì¼ëª…"), doc.metadata.get("chunk_idx"))
            if identifier not in seen_pairs:
                unique_docs.append(doc)
                seen_pairs.add(identifier)
        docs = unique_docs[:top_k]

        if rerank:
            docs = rerank_documents(query, docs, rerank_top_k, verbose)
        else:
            docs = docs[:top_k]

    else:
        raise ValueError(f"âŒ [Value] (retrieval.retrieve_documents.search_type) ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ì‹ì…ë‹ˆë‹¤: {search_type}")

    return docs
