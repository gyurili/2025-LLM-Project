import os
import yaml

from src.data_loader import data_load, data_process, data_chunking
from src.vector_db import generate_vector_db, load_vector_db


if __name__ == "__main__":
    try:
        # Config ë¶ˆëŸ¬ì˜¤ê¸°
        with open("config.yaml", "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        # ì„¤ì • í™•ì¸
        verbose = config["settings"].get("verbose", False)
        if not isinstance(verbose, bool):
            raise ValueError("âŒ(config.settings.verbose) verboseëŠ” True ë˜ëŠ” Falseì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if verbose:
            print("Verbose ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")

        # HWP íŒŒì¼ì„ PDFë¡œ ë³€í™˜
        folder_path = os.path.abspath(config["data"]["folder_path"])
        if not os.path.exists(folder_path):
            raise FileNotFoundError(f"âŒ(config.data.folder_path) í´ë” ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {folder_path}")
        # batch_convert_hwp_to_pdf(folder_path)

        # ë°ì´í„° ë¡œë“œ
        data_list_path = os.path.abspath(config["data"]["data_list_path"])
        if not os.path.exists(data_list_path):
            raise FileNotFoundError(f"âŒ(config.data.data_list_path) íŒŒì¼ ëª©ë¡ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_list_path}")
        limit = config["data"]["limit"] 
        if not isinstance(limit, int):
            raise ValueError("âŒ(config.data.limit) limitì€ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        df = data_load(data_list_path, limit=limit)
        print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")

        # ë°ì´í„° ì „ì²˜ë¦¬
        apply_ocr = config["data"].get("apply_ocr", False)
        if not isinstance(apply_ocr, bool):
            raise ValueError("âŒ(config.data.apply_ocr) apply_ocrëŠ” True ë˜ëŠ” Falseì—¬ì•¼ í•©ë‹ˆë‹¤.")
        file_type = config["data"]["type"]
        if file_type not in ["hwp", "pdf", "all"]:
            raise ValueError("âŒ(config.data.type) file_typeì€ 'hwp', 'pdf', 'all' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if verbose:
            print(f"íŒŒì¼ íƒ€ì…: {file_type}")
            print(f"OCR ì ìš© ì—¬ë¶€: {apply_ocr}")

        df = data_process(df, apply_ocr=apply_ocr, file_type=file_type)
        print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")

        # ì²­í¬ ìƒì„±
        splitter_type = config["chunk"]["splitter"]
        if not isinstance(splitter_type, str) or not splitter_type:
            raise ValueError("âŒ(config.chunk.splitter) ì²­í¬ ë¶„í• ê¸° íƒ€ì…ì€ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        chunk_size = config["chunk"]["size"]
        if chunk_size <= 0 or not isinstance(chunk_size, int):
            raise ValueError("âŒ(config.chunk.size) ì²­í¬ í¬ê¸°ëŠ” 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        chunk_overlap = config["chunk"]["overlap"]
        if chunk_overlap < 0 or not isinstance(chunk_overlap, int):
            raise ValueError("âŒ(config.chunk.overlap) ì²­í¬ ì˜¤ë²„ë©ì€ 0 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if verbose:
            print(f"ì²­í¬ í¬ê¸°: {chunk_size}")
            print(f"ì²­í¬ ì˜¤ë²„ë©: {chunk_overlap}")

        all_chunks = data_chunking(df=df, splitter_type=splitter_type, size=chunk_size, overlap=chunk_overlap)
        print("âœ… ì²­í¬ ìƒì„± ì™„ë£Œ")        

        # ë²¡í„° DB ìƒì„±
        embed_model = config["embedding"]["model"]
        if not isinstance(embed_model, str) or not embed_model:
            raise ValueError("âŒ(config.embedding.model) ì„ë² ë”© ëª¨ë¸ëª…ì´ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

        embeddings = generate_vector_db(all_chunks, embed_model)
        print("âœ… Vector DB ìƒì„±")

        # ë²¡í„° DB ë¡œë“œ
        vector_db_path = config["embedding"]["vector_db_path"]
        if not os.path.exists(vector_db_path):
            raise FileNotFoundError(f"âŒ(config.embedding.vector_db_path) ë²¡í„° DB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {vector_db_path}")
        
        vector_store = load_vector_db(vector_db_path, embed_model)
        print("âœ… Vector DB ë¡œë“œ")

        # ìœ ì‚¬ë„ ê²€ìƒ‰
        query = config["query"]["question"]
        if not isinstance(query, str):
            raise ValueError("âŒ(config.query.question) queryëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        top_k = config["query"]["top_k"]
        if not isinstance(top_k, int) or top_k <= 0:
            raise ValueError("âŒ(config.query.top_k) top_këŠ” 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if verbose:
            print(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
            print(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {top_k}")

        docs = vector_store.similarity_search(query, k=top_k)
        for i, doc in enumerate(docs, start=1):
            print(f"\nğŸ“„ ìœ ì‚¬ ë¬¸ì„œ {i}:\n{doc.page_content}")

    except FileNotFoundError as e:
        print(f"âŒ [FileNotFound] \n {e}")

    except ValueError as e:
        print(f"âŒ [Value] \n {e}")

    except TypeError as e:
        print(f"âŒ [Type] \n {e}")

    except yaml.YAMLError as e:
        print(f"âŒ [YAML] config.yaml íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: \n    {e}")

    except ImportError as e:
        print(f"âŒ [Import] ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: \n {e}")

    except Exception as e:
        print(f"âŒ [Runtime] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: \n {e}")