import os
import yaml

from src.data_loader import data_load, data_process, data_chunking
from src.vector_db import generate_vector_db, load_vector_db
from src.retriever import retrieve_documents
from src.generation import load_chat_model, build_qa_chain


if __name__ == "__main__":
    try:
        # Config ë¶ˆëŸ¬ì˜¤ê¸°
        with open("config.yaml", "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        # ì„¤ì • í™•ì¸
        if config["settings"]["verbose"]:
            print("Verbose ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            print(config)

        # HWP íŒŒì¼ì„ PDFë¡œ ë³€í™˜
        folder_path = os.path.abspath(config["data"]["folder_path"])
        # batch_convert_hwp_to_pdf(folder_path)

        # ë°ì´í„° ë¡œë“œ
        data_list_path = os.path.abspath(config["data"]["data_list_path"])
        if not os.path.exists(data_list_path):
            raise FileNotFoundError(f"âŒ íŒŒì¼ ëª©ë¡ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_list_path}")
        limit = config["data"]["limit"]
        if limit <= 0 or limit > 100 or not isinstance(limit, int):
            raise ValueError("âŒ limitì€ 1 ì´ìƒ 100 ì´í•˜ì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        df = data_load(data_list_path, limit=limit)
        print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")

        # ë°ì´í„° ì „ì²˜ë¦¬
        apply_ocr = config["data"]["apply_ocr"]
        if not isinstance(apply_ocr, bool):
            raise ValueError("âŒ apply_ocrëŠ” True ë˜ëŠ” Falseì—¬ì•¼ í•©ë‹ˆë‹¤.")
        file_type = config["data"]["type"]
        if file_type not in ["hwp", "pdf", "all"]:
            raise ValueError("âŒ file_typeì€ 'hwp', 'pdf', 'all' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if config["settings"]["verbose"]:
            print(f"íŒŒì¼ íƒ€ì…: {file_type}")
            print(f"OCR ì ìš© ì—¬ë¶€: {apply_ocr}")

        df = data_process(df, apply_ocr=apply_ocr, file_type=file_type)
        print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")

        # ì²­í¬ ìƒì„±
        splitter_type = config["chunk"]["splitter"]
        if splitter_type not in ["recursive", "token"]:
            raise ValueError("âŒ spliter_typeì€'recursive' ë˜ëŠ” 'token' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        chunk_size = config["chunk"]["size"]
        if chunk_size <= 0 or not isinstance(chunk_size, int):
            raise ValueError("âŒ ì²­í¬ í¬ê¸°ëŠ” 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        chunk_overlap = config["chunk"]["overlap"]
        if chunk_overlap < 0 or not isinstance(chunk_overlap, int):
            raise ValueError("âŒ ì²­í¬ ì˜¤ë²„ë©ì€ 0 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if config["settings"]["verbose"]:
            print(f"ì²­í¬ í¬ê¸°: {chunk_size}")
            print(f"ì²­í¬ ì˜¤ë²„ë©: {chunk_overlap}")

        all_chunks = data_chunking(df=df, splitter_type=splitter_type, size=chunk_size, overlap=chunk_overlap)
        print("âœ… ì²­í¬ ìƒì„± ì™„ë£Œ")        

        # ë²¡í„° DB ìƒì„±
        embed_model_name = config["embedding"]["model"]
        #âŒ ë‚˜ì¤‘ì— ë°©ì–´ì  ì½”ë“œ ì¶”ê°€í•´ì•¼í•¨

        embeddings = generate_vector_db(all_chunks, embed_model_name)
        print("âœ… Vector DB ìƒì„±")

        # ë²¡í„° DB ë¡œë“œ
        vector_db_path = config["embedding"]["vector_db_path"]
        embed_model = config["embedding"]["model"]
        if not os.path.exists(vector_db_path):
            raise FileNotFoundError(f"ë²¡í„° DB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {vector_db_path}")
        
        vector_store = load_vector_db(vector_db_path, embed_model)
        print("âœ… Vector DB ë¡œë“œ")

        # ìœ ì‚¬ë„ ê²€ìƒ‰
        query = config["query"]["question"]
        k = config["query"]["top_k"]
        search_type = config["retrieval"]["method"]

        if k <= 0:
            raise ValueError("këŠ” 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if config["settings"]["verbose"]:
            print(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
            print(f"ê²€ìƒ‰ ë°©ì‹: {search_type}, ê°œìˆ˜: {k}")
            
        docs = retrieve_documents(query, vector_store, k, search_type)
        for i, doc in enumerate(docs, 1):
            print(f"\nğŸ“„ ë¬¸ì„œ {i}")
            print(f"ë³¸ë¬¸:\n{doc['text'][:300]}...")
            print(f"ë©”íƒ€ë°ì´í„°: {doc['metadata']}")
        
        '''
        query = config["query"]["question"]
        k = config["query"]["top_k"]
        if k <= 0:
            raise ValueError("këŠ” 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if config["settings"]["verbose"]:
            print(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
            print(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {k}")

        docs = vector_store.similarity_search(query, k=k)
        for i, doc in enumerate(docs, start=1):
            print(f"\nğŸ“„ ìœ ì‚¬ ë¬¸ì„œ {i}:\n{doc.page_content}")
            
        
        # LLM ëª¨ë¸ ë¡œë“œ
        model_config = config["model"]
        query_config = config["query"]
        chat_model = load_chat_model(model_config)
        print("âœ… ìƒì„±í˜• LLM ë¡œë“œ")

        # QA ì²´ì¸ êµ¬ì„±
        qa_chain = build_qa_chain(
            vector_store=vector_store,
            chat_model=chat_model,
            top_k=query_config["top_k"]
        )

        query = query_config["question"]
        response = qa_chain.invoke(query)

        print(f"\nğŸ’¬ LLM ì‘ë‹µ:\n{response}")
        '''

    except Exception as e:
        print(f"âŒ Error: {e}")